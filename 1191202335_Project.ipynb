{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e89afa4-d74f-40fe-a62a-8574ee2d85af"
      },
      "source": [
        "# Brain Tumour Detection with Machine Learning\n",
        "##### Name: Ayat Abdulaziz Gaber Al-Khulaqi \n",
        "##### ID: 1191202335"
      ],
      "id": "3e89afa4-d74f-40fe-a62a-8574ee2d85af"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in data preprocessing in machine learning is to acquire the dataset. The dataset used to build and develop the Machine learning models is Brain Tumour Classification (MRI) from the Kaggle website (Brain Tumour Classification (MRI) | Kaggle). Machine learning models used are Transfer Learning and Convolutional Neural Networks (CNN)."
      ],
      "metadata": {
        "id": "5RCxsFnt3Vo6"
      },
      "id": "5RCxsFnt3Vo6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fa2339a-49bf-438c-aad6-cafa3eeda36a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ],
      "id": "3fa2339a-49bf-438c-aad6-cafa3eeda36a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f20e0bd-9636-4999-813f-f363ab55cb35"
      },
      "outputs": [],
      "source": [
        "# Helper libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "import pathlib\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "8f20e0bd-9636-4999-813f-f363ab55cb35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5buf1Iuqs4TG"
      },
      "outputs": [],
      "source": [
        "! unzip data.zip -d BarinMRI"
      ],
      "id": "5buf1Iuqs4TG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the dataset was acquired the second step is to import the data into the program. The tf.keras.utils.image_dataset_from_directory generates a tf.data.Dataset from image files in a directory. To create a tf.data.Dataset contains the batch size, image height, image width and object API. Since we have 4 directories in the testing and training folders, num_classes is initialised to 4. For the batch is set to 128 and the image width and height are set to 244. \n",
        "\n",
        "To get the object API is needed to use pathlib. To get the tf.data.Dataset is required to use tf.keras.utils.image_dataset_from_directory , then to set the values object API of each directory, image size and batch size. After running the code, it displays the number of files in all directories. \n"
      ],
      "metadata": {
        "id": "kdLskdf43eve"
      },
      "id": "kdLskdf43eve"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51b58f53-d133-4eae-abbe-8aeaa6fc70ea"
      },
      "outputs": [],
      "source": [
        "num_classes = 4\n",
        "batch_size = 128\n",
        "img_height = 224\n",
        "img_width = 224"
      ],
      "id": "51b58f53-d133-4eae-abbe-8aeaa6fc70ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKMVuq2cqNh5"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (img_height, img_width)"
      ],
      "id": "gKMVuq2cqNh5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3e4e9d7-d92c-4ddf-a233-1abf555ed82f"
      },
      "source": [
        "## Data Preperation"
      ],
      "id": "e3e4e9d7-d92c-4ddf-a233-1abf555ed82f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c087ce42-22b1-4a47-9206-5c093884feec"
      },
      "outputs": [],
      "source": [
        "Train_Path = pathlib.Path(\"/content/BarinMRI/Training\")\n",
        "Test_Path = pathlib.Path(\"/content/BarinMRI/Testing\")"
      ],
      "id": "c087ce42-22b1-4a47-9206-5c093884feec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2639d74-d00d-4641-9642-8383eecadc03"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    Train_Path,\n",
        "    shuffle=True,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)"
      ],
      "id": "e2639d74-d00d-4641-9642-8383eecadc03"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95b781a2-d9a5-4b84-b94c-15946c90bf23"
      },
      "outputs": [],
      "source": [
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    Test_Path,\n",
        "    shuffle=True,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)"
      ],
      "id": "95b781a2-d9a5-4b84-b94c-15946c90bf23"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530d0dee-c807-4fa7-ba63-6738a87ab2db"
      },
      "source": [
        "#### Class names in the train directory "
      ],
      "id": "530d0dee-c807-4fa7-ba63-6738a87ab2db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96d934e2-e5a4-417e-9e38-23f25ec5719f"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names"
      ],
      "id": "96d934e2-e5a4-417e-9e38-23f25ec5719f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53d35f68-ae58-46c1-ab62-7db58911321f"
      },
      "outputs": [],
      "source": [
        "print(class_names)"
      ],
      "id": "53d35f68-ae58-46c1-ab62-7db58911321f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721332d6-366f-463c-9a8d-63ed09b1037d"
      },
      "source": [
        "#### Number of classes in train set"
      ],
      "id": "721332d6-366f-463c-9a8d-63ed09b1037d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d83ce61b-1de8-4d27-8e05-f6879e71db37"
      },
      "outputs": [],
      "source": [
        "len(class_names)"
      ],
      "id": "d83ce61b-1de8-4d27-8e05-f6879e71db37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b48b980d-444c-488c-8d36-87ccccb68103"
      },
      "source": [
        "### Class distribution in training set"
      ],
      "id": "b48b980d-444c-488c-8d36-87ccccb68103"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the steps needed to display the class distribution for each dataset:\n",
        "Each dataset is unbatched, which splits the elements of a dataset into multiple elements. The unbatched removes all the batches and it gives all samples one after another.\n",
        "<br><br>\n",
        "Then as_numpy_iterator returns an iterator that converts all elements of the dataset to numpy. Using constructor list() to create an empty list, which creates a new list object. Np.array is used to convert a tensor to a numpy array. as_numpy_iterator  used to extract individual samples. Then list() is used to form a list of all individual samples, from the list we form a numpy array. \n",
        "<br><br>\n",
        "Np.unique finds the unique elements of an array.  After the first column of the numpy array which contains the class name  is used in Np.unique. The count is used in the second argument in Np.unique, which counts the number of images in each class.\n",
        "<br><br>\n",
        "Lastly, matplotlib.pyplot (plt)  is used to plot the bar graph for each dataset. \n"
      ],
      "metadata": {
        "id": "HlyqGmA03vJ8"
      },
      "id": "HlyqGmA03vJ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2d44f64-a29d-458d-9fbf-648e4f95544d"
      },
      "outputs": [],
      "source": [
        "ds = train_ds.unbatch()\n",
        "arr = np.array(list(ds.as_numpy_iterator()))"
      ],
      "id": "e2d44f64-a29d-458d-9fbf-648e4f95544d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1b6db67-7082-493a-b818-d1e0c1955907"
      },
      "outputs": [],
      "source": [
        "classes, counts = np.unique(arr[:, 1], return_counts=True)\n",
        "plt.figure(figsize=((5), (5)))\n",
        "plt.barh(class_names, counts)\n",
        "plt.title('Class distribution in training set')"
      ],
      "id": "d1b6db67-7082-493a-b818-d1e0c1955907"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aab66cba-0219-4815-8946-a009fcc7e19a"
      },
      "source": [
        "### Class distribution in testing set"
      ],
      "id": "aab66cba-0219-4815-8946-a009fcc7e19a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fb99d53-052a-42b1-a64d-5452ab334740"
      },
      "outputs": [],
      "source": [
        "ds2 = test_ds.unbatch()\n",
        "arr2 = np.array(list(ds2.as_numpy_iterator()))"
      ],
      "id": "2fb99d53-052a-42b1-a64d-5452ab334740"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b869ca40-97d6-470a-b92f-8f45f75d09f3"
      },
      "outputs": [],
      "source": [
        "classes, counts = np.unique(arr2[:, 1], return_counts=True)\n",
        "plt.figure(figsize=((5), (5)))\n",
        "plt.barh(class_names, counts)\n",
        "plt.title('Class distribution in testing set')"
      ],
      "id": "b869ca40-97d6-470a-b92f-8f45f75d09f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fffabec-d03e-45f0-9e99-c79691f72d72"
      },
      "source": [
        "### Displaying Images from Training Directory "
      ],
      "id": "3fffabec-d03e-45f0-9e99-c79691f72d72"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the first 25 images from the training and testing set and display the class name below each image. \n"
      ],
      "metadata": {
        "id": "lzlepd523_LT"
      },
      "id": "lzlepd523_LT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fb48ea0-0768-4a0e-a2d8-caaf389688d9"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(25):\n",
        "    ax = plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "id": "0fb48ea0-0768-4a0e-a2d8-caaf389688d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0427cc82-5dd4-4d6e-b713-46cd85bc006f"
      },
      "source": [
        "### Displaying Images from Testing Directory "
      ],
      "id": "0427cc82-5dd4-4d6e-b713-46cd85bc006f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6da7052a-2f95-4797-973b-44b8b974798f"
      },
      "outputs": [],
      "source": [
        "class_names = test_ds.class_names\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(25):\n",
        "    ax = plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "id": "6da7052a-2f95-4797-973b-44b8b974798f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99626a90-7513-432f-9861-4c829e271209"
      },
      "source": [
        "# Convolutional Neural Network(CNN) "
      ],
      "id": "99626a90-7513-432f-9861-4c829e271209"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conventional neural networks (CNN) is a supervised type of Deep learning, mainly used for image and speech recognition. In the sequential model includes: \n",
        "rescaling, 2D Convolution, 2D Max Pooling, Flattening and dense layers. "
      ],
      "metadata": {
        "id": "0MCq6kw44Spz"
      },
      "id": "0MCq6kw44Spz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f072a8f-4bd9-43dd-bb45-d3e1be5975a4"
      },
      "source": [
        "### Setting up the layers "
      ],
      "id": "7f072a8f-4bd9-43dd-bb45-d3e1be5975a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05bd0fee-97b9-4f12-993e-14f346dfebcb"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential([\n",
        "    layers.Rescaling(1./255),\n",
        "    layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(256, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(512, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(512, (3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(num_classes)\n",
        "])"
      ],
      "id": "05bd0fee-97b9-4f12-993e-14f346dfebcb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **2D convolution** block is a layer which can be used to detect spatial features in an image, either working directly on the image data or on the output of previous convolution blocks.  Every block consists of a number of filters, where each filter is a height * width * channels matrix of trainable weights. The filter channel is 3 * 3 ( height by width).\n",
        "<br><br>\n",
        "**Flatten** the layer and reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. \n",
        "<br><br>\n",
        "**2D max pooling** reduces the number of parameters, the size of data, the amount of computation needed, and it controls overfitting. The max pooling block moves a rectangle over the incoming data, selecting the maximum in each certain window. \n"
      ],
      "metadata": {
        "id": "gnRqgjSS4u13"
      },
      "id": "gnRqgjSS4u13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37170917-66ad-41f7-849d-f3de7d08bf2b"
      },
      "source": [
        "### Setting up hyperparameters"
      ],
      "id": "37170917-66ad-41f7-849d-f3de7d08bf2b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c294703f-26cc-4c0f-bac1-c840ae0562cf"
      },
      "outputs": [],
      "source": [
        "model1.compile(optimizer='adam', \n",
        "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "             metrics=['accuracy'])\n",
        "\n"
      ],
      "id": "c294703f-26cc-4c0f-bac1-c840ae0562cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCFgL2Mnd5vg"
      },
      "source": [
        "### Training the model"
      ],
      "id": "OCFgL2Mnd5vg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b4b335b-3dc5-444c-8b63-74256422ee1f"
      },
      "outputs": [],
      "source": [
        "history = model1.fit(train_ds, epochs=50,validation_data=(test_ds))"
      ],
      "id": "7b4b335b-3dc5-444c-8b63-74256422ee1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk0fZLYVVCb0"
      },
      "outputs": [],
      "source": [
        "print(\"PERFOMANCE\",model1.evaluate(test_ds))"
      ],
      "id": "uk0fZLYVVCb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MLk6WxHd61t"
      },
      "source": [
        "### Model Summary"
      ],
      "id": "7MLk6WxHd61t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recalling layer parameter is zero because the layer does not learn anything, however, it rescales and offsets the values of a batch of the image, the rescaling is equal 1./255 as illustrated in sequential model so the  inputs are in the [0, 1] range. To calculate the number of parameters in the 2D convolution layer it needs the input channel number multiplied by kernel height multiplied by kernel width added by 1, and lastly, the final number of parameters is multiplied by the output channels number. "
      ],
      "metadata": {
        "id": "E87eRrIm5lhq"
      },
      "id": "E87eRrIm5lhq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecacaa92-b703-4c12-859b-7f399c5b0c59"
      },
      "outputs": [],
      "source": [
        "model1.summary()"
      ],
      "id": "ecacaa92-b703-4c12-859b-7f399c5b0c59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_OnRoHeAM8"
      },
      "source": [
        "### Plot Training History"
      ],
      "id": "Fe_OnRoHeAM8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aa4f52a-4c47-4446-aa95-9804424feb04"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "id": "3aa4f52a-4c47-4446-aa95-9804424feb04"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StEDft3heBEc"
      },
      "source": [
        "### Training and Validation Accuracy"
      ],
      "id": "StEDft3heBEc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bfe708-d56f-4961-8878-5e9fd2b25e39"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "id": "33bfe708-d56f-4961-8878-5e9fd2b25e39"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFZZeENueD8F"
      },
      "source": [
        "### Training and Validation Loss"
      ],
      "id": "ZFZZeENueD8F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3dbb522-8fce-40d2-ac56-d24fb159983f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('loss')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n"
      ],
      "id": "d3dbb522-8fce-40d2-ac56-d24fb159983f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee9394f9-d332-440e-b3bf-0fe82ba8c54a"
      },
      "source": [
        "# Transfer Learning "
      ],
      "id": "ee9394f9-d332-440e-b3bf-0fe82ba8c54a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5b23102-e4cc-4b10-b0d8-dad6ba05b12a"
      },
      "source": [
        "## MobileNetV2 "
      ],
      "id": "c5b23102-e4cc-4b10-b0d8-dad6ba05b12a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNetV2 is a convolutional neural network architecture for image classification that was developed by Google.\n",
        "<br><br>\n",
        "For transfer learning it’s using tensorflow.keras.applications to import MobileNetV2.  Keras applications are deep learning models that are made available alongside pre-trained weights.  Preprocess input is a tensor or Numpy array encoding a batch of images. \n"
      ],
      "metadata": {
        "id": "mLIFAEZs8OIC"
      },
      "id": "mLIFAEZs8OIC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvGcbjdqR_Uh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
      ],
      "id": "jvGcbjdqR_Uh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQEIjQeLeLVz"
      },
      "source": [
        "### Augmentation for training set"
      ],
      "id": "FQEIjQeLeLVz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "From tf.keras.preprocessing.image, ImageDataGenerator is used to generate batches of tensor image data with real-time data augmentation. The images are being flipped horizontally and in comments are ways to augment the data.\n",
        "<br><br>\n",
        "The Train generator variable it’s using the train data generator, first of coding. However, the test it’s using the test data generator which does not do data augmentation, the second part of coding. Lastly, setting up the train, and test generators, which use the flow from the directory and then provide the directory, target size, color mode, class mode, and batch size.  \n"
      ],
      "metadata": {
        "id": "w-fAO-eC7tm8"
      },
      "id": "w-fAO-eC7tm8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HBx1D44KeWj"
      },
      "outputs": [],
      "source": [
        "train_data_generator = image.ImageDataGenerator(\n",
        "                                            horizontal_flip=True,\n",
        "#                                             zoom_range=0.15,\n",
        "#                                             width_shift_range=0.2,\n",
        "#                                             height_shift_range=0.2,\n",
        "#                                             shear_range=0.15,\n",
        "                                            preprocessing_function=preprocess_input\n",
        "                                            )"
      ],
      "id": "4HBx1D44KeWj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ9OsTJjKduS"
      },
      "outputs": [],
      "source": [
        "# No Augmentation for valid and test\n",
        "test_data_generator = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "\n",
        "train_generator = train_data_generator.flow_from_directory(directory= Train_Path,\n",
        "                                                    target_size=IMAGE_SIZE,\n",
        "                                                    color_mode= 'rgb',\n",
        "                                                    class_mode= 'categorical',\n",
        "                                                    batch_size= batch_size)\n",
        "\n",
        "test_generator = test_data_generator.flow_from_directory(directory= Test_Path,\n",
        "                                                    target_size=IMAGE_SIZE,\n",
        "                                                    color_mode= 'rgb',\n",
        "                                                    class_mode= 'categorical',\n",
        "                                                    batch_size= batch_size)"
      ],
      "id": "NJ9OsTJjKduS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9eInL_eOHJ"
      },
      "source": [
        "### Build The Transfer Model"
      ],
      "id": "OF9eInL_eOHJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base is initialized to MonileNetV2 and it contains weights and includes top args. The weights are a string initialized to imagenet which is pre-training on ImageNet. Include top is a boolean initialized to false to not include the fully-connected layer at the top of the network. Then we set the base to be not trainable, so all children layers become non-trainable as well. The sequential model it’s using the base, global average pooling 2D, dense, and dropout layers."
      ],
      "metadata": {
        "id": "r1mx9DbS8Gz9"
      },
      "id": "r1mx9DbS8Gz9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8cvkKcWS9JS"
      },
      "outputs": [],
      "source": [
        "base = MobileNetV2(weights='imagenet', include_top=False)\n",
        "base.trainable = False\n",
        "\n",
        "model2 = Sequential([\n",
        "    base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(num_classes)\n",
        "])"
      ],
      "id": "c8cvkKcWS9JS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **global average pooling 2D layer** takes a tensor of the size input width * input height * input channels and computes the average value of all values across the whole matrix for each of the input channels. \n",
        "<br><br>\n",
        "The **dense layer** is used to create fully connected layers, in which every output depends on every input\n",
        "<br><br>\n",
        "The **dropout layer** randomly sets the outgoing edges of hidden units to 0 at each update of the training phase, which is used to prevent the possibility of  overfitting"
      ],
      "metadata": {
        "id": "lE3HAbgX8VpG"
      },
      "id": "lE3HAbgX8VpG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GHqeqWtfxtI"
      },
      "source": [
        "### Setting up hyperparameters"
      ],
      "id": "4GHqeqWtfxtI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the transfer learning model, before that it needs a few more settings :\n",
        "\n",
        "- Optimizer is an algorithm for adjusting the inner parameters of the model in order to minimise loss.\n",
        "<br>\n",
        "- Loss function is an algorithm for measuring how far the model's outputs are from the desired output. The from logits initialised to true indicates that the values of the loss obtained by the model are not normalised, also it is used when we don't have any softmax function in our model.\n",
        "<br>\n",
        "- Metrics is used for monitoring the training and testing steps. \n"
      ],
      "metadata": {
        "id": "_Bk5IwyZ9AGf"
      },
      "id": "_Bk5IwyZ9AGf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UngGxnDfS-SG"
      },
      "outputs": [],
      "source": [
        "model2.compile(\n",
        "    optimizer=\"Adam\",\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "id": "UngGxnDfS-SG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhZw1iemeR7-"
      },
      "source": [
        "### Training the model"
      ],
      "id": "MhZw1iemeR7-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER4bWao5TBns"
      },
      "outputs": [],
      "source": [
        "history2 = model2.fit(train_generator, validation_data=test_generator, epochs=50)\n",
        "print(\"PERFOMANCE\",model2.evaluate(test_generator))\n"
      ],
      "id": "ER4bWao5TBns"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK5Ow6oXeY1T"
      },
      "source": [
        "### Model Summary"
      ],
      "id": "vK5Ow6oXeY1T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPnoOf9GTBMo"
      },
      "outputs": [],
      "source": [
        "model2.summary()"
      ],
      "id": "oPnoOf9GTBMo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWZQ4s5eb_v"
      },
      "source": [
        "### Plot Training History"
      ],
      "id": "KpWZQ4s5eb_v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eTn5PeZdx3a"
      },
      "outputs": [],
      "source": [
        "acc = history2.history['accuracy']\n",
        "val_acc = history2.history['val_accuracy']\n",
        "\n",
        "loss = history2.history['loss']\n",
        "val_loss = history2.history['val_loss']"
      ],
      "id": "2eTn5PeZdx3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KpNP6mfed9f"
      },
      "source": [
        "### Training and Validation Accuracy"
      ],
      "id": "9KpNP6mfed9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrmfOM5Yd2pO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "id": "GrmfOM5Yd2pO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXPtSqrWekfH"
      },
      "source": [
        "### Training and Validation Loss"
      ],
      "id": "fXPtSqrWekfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dnWYHHud4Dz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "id": "7dnWYHHud4Dz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGJubXb7Q5PR"
      },
      "source": [
        "## VGG19"
      ],
      "id": "WGJubXb7Q5PR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG19 is a convolutional neural network model trained on the ImageNet dataset. This model and its variants are used for image classification and other vision tasks. The \"19\" in the name refers to the number of weight layers in the network. VGG19 consists of 19 weight layers, which include 16 convolutional layers and three fully-connected layers. The convolutional layers are arranged in a series of convolutional blocks. Each block contains two or three convolutional layers and a max pooling layer. The fully-connected layers follow the convolutional layers and are used for classification. \n",
        "<br><br>\n",
        "Using tensorflow.keras.applications to import VGG19 architecture, which is a convolutional neural network that is 19 layers deep.\n"
      ],
      "metadata": {
        "id": "ueuX87xj91Lm"
      },
      "id": "ueuX87xj91Lm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep8gZD6lQ5PR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input"
      ],
      "id": "ep8gZD6lQ5PR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J67PQvcSKsKi"
      },
      "outputs": [],
      "source": [
        "train_data_generator = image.ImageDataGenerator(\n",
        "                                            horizontal_flip=True,\n",
        "#                                             zoom_range=0.15,\n",
        "#                                             width_shift_range=0.2,\n",
        "#                                             height_shift_range=0.2,\n",
        "#                                             shear_range=0.15,\n",
        "                                            preprocessing_function=preprocess_input\n",
        "                                            )"
      ],
      "id": "J67PQvcSKsKi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qplLJrlveuHT"
      },
      "source": [
        "### Augmentation for training set"
      ],
      "id": "qplLJrlveuHT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AozcnICMKsKj"
      },
      "outputs": [],
      "source": [
        "# No Augmentation for valid and test\n",
        "test_data_generator = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "\n",
        "train_generator = train_data_generator.flow_from_directory(directory= Train_Path,\n",
        "                                                    target_size=IMAGE_SIZE,\n",
        "                                                    color_mode= 'rgb',\n",
        "                                                    class_mode= 'categorical',\n",
        "                                                    batch_size= batch_size)\n",
        "\n",
        "test_generator = test_data_generator.flow_from_directory(directory= Test_Path,\n",
        "                                                    target_size=IMAGE_SIZE,\n",
        "                                                    color_mode= 'rgb',\n",
        "                                                    class_mode= 'categorical',\n",
        "                                                    batch_size= batch_size)"
      ],
      "id": "AozcnICMKsKj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxnMhwjjQ5PT"
      },
      "source": [
        "### Build The Transfer Model"
      ],
      "id": "KxnMhwjjQ5PT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noX9I9qZQ5PT"
      },
      "outputs": [],
      "source": [
        "base = VGG19(weights='imagenet', include_top=False)\n",
        "base.trainable = False\n",
        "\n",
        "model3 = Sequential([\n",
        "    base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(num_classes)\n",
        "])"
      ],
      "id": "noX9I9qZQ5PT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDA-9aTwf7-9"
      },
      "source": [
        "### Setting up hyperparameters"
      ],
      "id": "SDA-9aTwf7-9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsx8mjgrQ5PT"
      },
      "outputs": [],
      "source": [
        "model3.compile(\n",
        "    optimizer=\"Adam\",\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "id": "Gsx8mjgrQ5PT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWldp_OXewam"
      },
      "source": [
        "### Training the model"
      ],
      "id": "uWldp_OXewam"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml15kCOKVPp2"
      },
      "outputs": [],
      "source": [
        "history3 = model3.fit(train_generator, validation_data=test_generator, epochs=50)\n",
        "print(\"PERFOMANCE\",model3.evaluate(test_generator))"
      ],
      "id": "Ml15kCOKVPp2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEC8yTzRe1JX"
      },
      "source": [
        "### Model Summary"
      ],
      "id": "HEC8yTzRe1JX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9QO6X1DQ5PT"
      },
      "outputs": [],
      "source": [
        "model3.summary()"
      ],
      "id": "j9QO6X1DQ5PT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7KLd-QpQ5PU"
      },
      "source": [
        "### Plot Training History"
      ],
      "id": "O7KLd-QpQ5PU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQZHOteRQ5PU"
      },
      "outputs": [],
      "source": [
        "acc = history3.history['accuracy']\n",
        "val_acc = history3.history['val_accuracy']\n",
        "\n",
        "loss = history3.history['loss']\n",
        "val_loss = history3.history['val_loss']"
      ],
      "id": "lQZHOteRQ5PU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH4bQniGQ5PU"
      },
      "source": [
        "### Training and Validation Accuracy"
      ],
      "id": "lH4bQniGQ5PU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkk7_DaIQ5PU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "id": "Bkk7_DaIQ5PU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZj1RRy9Q5PU"
      },
      "source": [
        "### Training and Validation Loss"
      ],
      "id": "mZj1RRy9Q5PU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnggXjndQ5PU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "id": "bnggXjndQ5PU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJZP5BNNq2A1"
      },
      "source": [
        "## ResNet101V2"
      ],
      "id": "sJZP5BNNq2A1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet101V2 is an image recognition convolutional neural network trained to distinguish objects in images. It is a subset of the ResNet101 network, which is a more in-depth version of the original ResNet network. The \"V2\" in the name alludes to an improved version of the original ResNet101 network.\n",
        "\n"
      ],
      "metadata": {
        "id": "DIXC19wC-TlP"
      },
      "id": "DIXC19wC-TlP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xaoalTOq2A1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet101V2\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input"
      ],
      "id": "1xaoalTOq2A1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tk4uCttfN9C"
      },
      "source": [
        "### Augmentation for training set"
      ],
      "id": "2Tk4uCttfN9C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDAixOIHKuGk"
      },
      "outputs": [],
      "source": [
        "train_data_generator = image.ImageDataGenerator(\n",
        "                                            horizontal_flip=True,\n",
        "#                                             zoom_range=0.15,\n",
        "#                                             width_shift_range=0.2,\n",
        "#                                             height_shift_range=0.2,\n",
        "#                                             shear_range=0.15,\n",
        "                                            preprocessing_function=preprocess_input\n",
        "                                            )"
      ],
      "id": "eDAixOIHKuGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOtLGahmKuGl"
      },
      "outputs": [],
      "source": [
        "# No Augmentation for valid and test\n",
        "test_data_generator = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "\n",
        "train_generator = train_data_generator.flow_from_directory(directory= Train_Path,\n",
        "                                                    target_size=IMAGE_SIZE,\n",
        "                                                    color_mode= 'rgb',\n",
        "                                                    class_mode= 'categorical',\n",
        "                                                    batch_size= batch_size)\n",
        "\n",
        "test_generator = test_data_generator.flow_from_directory(directory= Test_Path,\n",
        "                                                    target_size=IMAGE_SIZE,\n",
        "                                                    color_mode= 'rgb',\n",
        "                                                    class_mode= 'categorical',\n",
        "                                                    batch_size= batch_size)"
      ],
      "id": "NOtLGahmKuGl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ionZxKRHq2A2"
      },
      "source": [
        "### Build The Transfer Model"
      ],
      "id": "ionZxKRHq2A2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sPgyY-Wq2A2"
      },
      "outputs": [],
      "source": [
        "base = ResNet101V2(weights='imagenet', include_top=False)\n",
        "base.trainable = False\n",
        "\n",
        "model4 = Sequential([\n",
        "    base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(num_classes)\n",
        "])"
      ],
      "id": "1sPgyY-Wq2A2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChnTyRUuf-0-"
      },
      "source": [
        "### Setting up hyperparameters"
      ],
      "id": "ChnTyRUuf-0-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tAzfJYXq2A2"
      },
      "outputs": [],
      "source": [
        "model4.compile(\n",
        "    optimizer=\"Adam\",\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "id": "1tAzfJYXq2A2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8AdPms0fRGK"
      },
      "source": [
        "### Training the model"
      ],
      "id": "W8AdPms0fRGK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZN8QKlyq2A2"
      },
      "outputs": [],
      "source": [
        "history4 = model4.fit(train_generator, validation_data=test_generator, epochs=50)\n",
        "print(\"PERFOMANCE\",model4.evaluate(test_generator))"
      ],
      "id": "yZN8QKlyq2A2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeSeiaCGfXED"
      },
      "source": [
        "### Model Summary"
      ],
      "id": "PeSeiaCGfXED"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf5QbfZCq2A2"
      },
      "outputs": [],
      "source": [
        "model4.summary()"
      ],
      "id": "mf5QbfZCq2A2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7uUa6yDq2A2"
      },
      "source": [
        "### Plot Training History"
      ],
      "id": "-7uUa6yDq2A2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKN0dT3Sq2A3"
      },
      "outputs": [],
      "source": [
        "acc = history4.history['accuracy']\n",
        "val_acc = history4.history['val_accuracy']\n",
        "\n",
        "loss = history4.history['loss']\n",
        "val_loss = history4.history['val_loss']"
      ],
      "id": "ZKN0dT3Sq2A3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt90_MkXq2A3"
      },
      "source": [
        "### Training and Validation Accuracy"
      ],
      "id": "Gt90_MkXq2A3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxxU2kkaq2A3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "id": "qxxU2kkaq2A3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qcMTs-Wq2A3"
      },
      "source": [
        "### Training and Validation Loss"
      ],
      "id": "_qcMTs-Wq2A3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uELOAfujq2A3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "id": "uELOAfujq2A3"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e3e4e9d7-d92c-4ddf-a233-1abf555ed82f",
        "721332d6-366f-463c-9a8d-63ed09b1037d",
        "c5b23102-e4cc-4b10-b0d8-dad6ba05b12a",
        "WGJubXb7Q5PR",
        "sJZP5BNNq2A1"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}